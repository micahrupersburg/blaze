input {
  exec{ # prod yarn cluster metrics
    command => "curl -H'Accept: application/xml' 'http://p3plpaname02.prod.phx3.gdg:8088/ws/v1/cluster/metrics'  | ./parse_cluster_metrics.py"
    interval => 300
    codec=>line
    type => "yarn_cluster_metrics"
    add_field => {cluster => "prod" "hostname" => "p3plpaname02.prod.phx3.gdg"}
  }


  exec { # old prod ganglia
    command => "./gparse.py p3plpaname02.prod.phx3.gdg"
    interval => 300
    codec => json_lines{}
    type => "ganglia"
    add_field => {cluster => "prod" ganglia_source => "p3plpashl03.prod.phx3.gdg"}
  }

  exec { # prod ganglia
    command => "./gparse.py"
    interval => 300
    codec => json_lines{}
    type => "ganglia"
    add_field => {cluster => "prod" ganglia_source => "p3plpashl03.prod.phx3.gdg"}
  }

  exec { # kafka ganglia
    command => "./gparse.py p3plpashl03.prod.phx3.gdg 8659"
    interval => 300
    codec => json_lines{}
    type => "ganglia"
    add_field => {cluster => "prod" ganglia_source => "p3plpashl03.prod.phx3.gdg"}
  }

  exec { # prod feed_info
    command => "./hive_cmd.sh hive -f feed_db.contents.query | ./feed_status_parse.py"
    interval => 300
    codec => json_lines{}
    type => "hive_feed_info"
    add_field => {cluster => "prod"}
  }

  exec { # mws prod
    command => "./mws/mwsload.py p3plpa %lcds"
    interval => 300 #5 minutes
    codec => json_lines{}
    type => "mws_dump"
    add_field => {cluster => "prod"}
  }

  exec { #poll du for prod cluster
    command => "./prod_cmd.sh hdfs dfs -du /feed-data/ /user/ /feed-data/dcr/ /afternic /app-logs /apps /audit /c3 /conf /datamgmt /dbmarketing /domains /hbase /hdw /hosting /hosting_cpanel /hosting_plesk /hosting_stats /hosting_wpaas /locu /mapred /mysqlsqooper /o365 /qsc /reference /system /teradata /tms /traffic_analyzer /verisign /wopr /hbase/data/default | ./parse_du.py"
    interval => 3600 #1 hour
    codec => json_lines{}
    type => "hdfs_du"
    add_field => {cluster => "prod" }
  }

  file {
    path => "/var/log/nagios/nagios.log"
    type => "nagios"
    add_field => {cluster => "prod" }
  }

}

filter {
  if([type] == "hdfs_du") {
    if([directory] == "/feed-data/dcr") { # drop just dcr directory
      drop{}
    }
  }

  if([type] == "hdfs_audit") {
    kv {
      field_split=> "\t"
      include_keys=> ["allowed", "ugi", "ip", "cmd", "src", "dst", "perm"]
      remove_field => ["message", "command"]
    }
    if([src] =~ "^/tmp" or [ugi] =~ "^hbase/" or [cmd] in ["getfileinfo", "listStatus", "open"]) {
      drop{}
    }

  }
  if([type] == "ganglia") {
    date {
      locale => en
      match => ["metric_ts", "UNIX"]
    }
  }

  if([type] == "nagios") {
    grok {
      match => {message => "%{NAGIOSLOGLINE}"}
      remove_field => ["message", "command"]
    }

    date {
      locale => en
      match => ["nagios_epoch", "UNIX"]
    }
    if("_grokparsefailure" in [tags]) {
      mutate { add_field => {"grok_failure" => true}}
    }
  }
  if([type] == "yarn_cluster_metrics") {
    csv {
      columns => ["name", "value", "units"]
      remove_field => ["message", "command"]
      add_field => {"group" => "yarn.cluster.metrics"}
    }
    mutate {
      convert => [ "value", "float" ]
    }
  }
}

output {
  if([type] == "ganglia") {
    elasticsearch {
      host => "p3plpashl03.prod.phx3.gdg"
      port => 8080
      index_type => "%{type}"
      document_id => "%{hostname}_%{full_name}_%{metric_ts}"
      protocol => "http"
      cluster => "hadoop-ops-es"
    }

  } else if([type] == "mws_dump") {

    elasticsearch {
      host => "p3plpashl03.prod.phx3.gdg"
      port => 8080
      index_type => "%{type}"
      document_id => "%{id}"
      protocol => "http"
      cluster => "hadoop-ops-es"
    }

  } else {

  elasticsearch{
    host => "p3plpashl03.prod.phx3.gdg"
    port => 8080
    index_type => "%{type}"
    protocol => "http"
    cluster => "hadoop-ops-es"
  }

}
if([type] == "fake") {
  stdout {codec=> rubydebug}
}
}
